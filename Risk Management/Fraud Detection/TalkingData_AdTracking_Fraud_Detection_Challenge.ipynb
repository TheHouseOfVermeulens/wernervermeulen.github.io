{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TalkingData AdTracking Fraud Detection Challenge.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheHouseOfVermeulens/wernervermeulen.github.io/blob/master/TalkingData_AdTracking_Fraud_Detection_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncD3w-hUGxSZ",
        "colab_type": "text"
      },
      "source": [
        "# DESCRIPTION: \n",
        "Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest\n",
        "mobile market in the world and therefore suffers from huge volumes of fradulent traffic.\n",
        "\n",
        "[TalkingData](https://www.talkingdata.com), China‚Äôs largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user‚Äôs click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.\n",
        "\n",
        "While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. Challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHw45xVHVRD",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION: \n",
        "The goal of the case is to predict if a user will download an app after clicking through an ad. For this case I have used a small sample of the data, dropping 99% of negative records (where the app wasn't downloaded) to make the target more balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zACj4cJGuOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "click_data = pd.read_csv('../input/feature-engineering-data/train_sample.csv',\n",
        "                         parse_dates=['click_time'])\n",
        "click_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VigQbneaSlXw",
        "colab_type": "text"
      },
      "source": [
        "Out[1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIXTaxM4Q-mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      ip \t app \tdevice \tos \tchannel \tclick_time \t       attributed_time \tis_attributed\n",
        "0 \t89489 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:13:23 \tNaN                 \t0\n",
        "1 \t204158 \t35 \t 1 \t    13 \t  21 \t   2017-11-06 15:41:07 \t2017-11-07 08:17:19 \t1\n",
        "2 \t3437 \t   6 \t 1 \t    13 \t 459 \t   2017-11-06 15:42:32 \tNaN \t                0\n",
        "3 \t167543 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:56:17 \tNaN \t                0\n",
        "4 \t147509 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:57:01 \tNaN \t                0\n",
        "5 \t71421 \t15 \t 1 \t    13 \t 153 \t   2017-11-06 16:00:00 \tNaN \t                0\n",
        "6 \t76953 \t14 \t 1 \t    13 \t 379 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "7 \t187909 \t 2 \t 1 \t    25 \t 477 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "8 \t116779 \t 1 \t 1 \t     8 \t 150 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "9 \t47857 \t 3 \t 1 \t    15 \t 205 \t   2017-11-06 16:00:01 \tNaN \t                0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUNRxXiXSDA1",
        "colab_type": "text"
      },
      "source": [
        "## Baseline Model\n",
        "\n",
        "The first thing I need to do is construct a baseline model. All new features, processing, encodings, and feature selection should improve upon this baseline model. First I need to do a bit of feature engineering before training the model itself.\n",
        "\n",
        "### 1) Features from timestamps\n",
        "From the timestamps, create features for the day, hour, minute and second. Store these as new integer columns `day`, `hour`, `minute`, and `second` in a new DataFrame `clicks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TQN6mj4Sb_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split up the times\n",
        "click_times = click_data['click_time']\n",
        "clicks['day'] = click_times.dt.day.astype('uint8')\n",
        "clicks['hour'] = click_times.dt.hour.astype('uint8')\n",
        "clicks['minute'] = click_times.dt.minute.astype('uint8')\n",
        "clicks['second'] = click_times.dt.second.astype('uint8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlwR60nkUBbw",
        "colab_type": "text"
      },
      "source": [
        "### 2) Label Encoding\n",
        "For each of the categorical features `['ip', 'app', 'device', 'os', 'channel']`, I use scikit-learn's `LabelEncoder` to create new features in the `clicks` DataFrame. The new column names should be the original column name with `'_labels'` appended, like `ip_labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awKXKURxUC-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Create new columns in clicks using preprocessing.LabelEncoder()\n",
        "for feature in cat_features:\n",
        "    encoded = label_encoder.fit_transform(clicks[feature])\n",
        "    clicks[feature + '_labels'] = encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NnekqmU8bB",
        "colab_type": "text"
      },
      "source": [
        "### 3) One-hot Encoding\n",
        "\n",
        "Now I have label encoded features, does it make sense to use one-hot encoding for the categorical variables ip, app, device, os, or channel?\n",
        "\n",
        "Answer: The ip column has 58,000 values, which means it will create an extremely sparse matrix with 58,000 columns. This many columns will make your model run very slow, so in general you want to avoid one-hot encoding features with many levels. LightGBM models work with label encoded features, so you don't actually need to one-hot encode the categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLZwYyIvVadt",
        "colab_type": "text"
      },
      "source": [
        "## Train, validation, and test sets\n",
        "With my baseline features ready, we need to split our data into training and validation sets. I should also hold out a test set to measure the final accuracy of the model.\n",
        "\n",
        "### 4) Train/test splits with time series data\n",
        "This is time series data. Are there any special considerations when creating train/test splits for time series? If so, what and why?\n",
        "\n",
        "Answer: Since our model is meant to predict events in the future, we must also validate the model on events in the future. If the data is mixed up between the training and test sets, then future data will leak in to the model and our validation results will overestimate the performance on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwecMtv8Wbpq",
        "colab_type": "text"
      },
      "source": [
        "### Create train/validation/test splits\n",
        "\n",
        "Here I'll create training, validation, and test splits. First, `clicks` DataFrame is sorted in order of increasing time. The first 80% of the rows are the train set, the next 10% are the validation set, and the last 10% are the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8xwIUeGVcTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_cols = ['day', 'hour', 'minute', 'second', \n",
        "                'ip_labels', 'app_labels', 'device_labels',\n",
        "                'os_labels', 'channel_labels']\n",
        "\n",
        "valid_fraction = 0.1\n",
        "clicks_srt = clicks.sort_values('click_time')\n",
        "valid_rows = int(len(clicks_srt) * valid_fraction)\n",
        "train = clicks_srt[:-valid_rows * 2]\n",
        "# valid size == test size, last two sections of the data\n",
        "valid = clicks_srt[-valid_rows * 2:-valid_rows]\n",
        "test = clicks_srt[-valid_rows:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjyYNDlYWvK0",
        "colab_type": "text"
      },
      "source": [
        "### Train with LightGBM\n",
        "\n",
        "Now I can create LightGBM dataset objects for each of the smaller datasets and train the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiTvHy4W2Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
        "dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
        "dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
        "\n",
        "param = {'num_leaves': 64, 'objective': 'binary'}\n",
        "param['metric'] = 'auc'\n",
        "num_round = 1000\n",
        "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10x0V6yZXVtU",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model\n",
        "Finally, with the model trained, I'll evaluate it's performance on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amApWllSXW1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "ypred = bst.predict(test[feature_cols])\n",
        "score = metrics.roc_auc_score(test['is_attributed'], ypred)\n",
        "print(f\"Test score: {score}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZeBchTXjJP",
        "colab_type": "text"
      },
      "source": [
        "Out[17]\n",
        "Test score: 0.9726727334566094"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CVWl5uSX3gZ",
        "colab_type": "text"
      },
      "source": [
        "This is my baseline score for the model. When I transform features, add new ones, or perform feature selection, I should be improving on this score. However, since this is the test set, I only want to look at it at the end of all our manipulations. At the very end of this case I'll look at the test score again to see if I've improved on the baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ovoDYVsdR1Y",
        "colab_type": "text"
      },
      "source": [
        "Now I can apply more advanced encodings to encode the categorical variables to improve your classifier model. The encodings I will implement are:\n",
        "\n",
        "- Count Encoding\n",
        "- Target Encoding\n",
        "- Leave-one-out Encoding\n",
        "- CatBoost Encoding\n",
        "- Feature embedding with SVD  \n",
        "\n",
        "Below I'll define a couple functions to help test the new encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqb0qrbpfxVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_splits(dataframe, valid_fraction=0.1):\n",
        "    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n",
        "        the column 'click_time'. Set the size of the validation and test sets with\n",
        "        the valid_fraction keyword argument.\n",
        "    \"\"\"\n",
        "\n",
        "    dataframe = dataframe.sort_values('click_time')\n",
        "    valid_rows = int(len(dataframe) * valid_fraction)\n",
        "    train = dataframe[:-valid_rows * 2]\n",
        "    # valid size == test size, last two sections of the data\n",
        "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
        "    test = dataframe[-valid_rows:]\n",
        "    \n",
        "    return train, valid, test\n",
        "\n",
        "def train_model(train, valid, test=None, feature_cols=None):\n",
        "    if feature_cols is None:\n",
        "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
        "                                           'is_attributed'])\n",
        "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
        "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
        "    \n",
        "    param = {'num_leaves': 64, 'objective': 'binary', \n",
        "             'metric': 'auc', 'seed': 7}\n",
        "    num_round = 1000\n",
        "    print(\"Training model!\")\n",
        "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
        "                    early_stopping_rounds=20, verbose_eval=False)\n",
        "    \n",
        "    valid_pred = bst.predict(valid[feature_cols])\n",
        "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
        "    print(f\"Validation AUC score: {valid_score}\")\n",
        "    \n",
        "    if test is not None: \n",
        "        test_pred = bst.predict(test[feature_cols])\n",
        "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
        "        return bst, valid_score, test_score\n",
        "    else:\n",
        "        return bst, valid_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp0HWWNmgifs",
        "colab_type": "text"
      },
      "source": [
        "# Note here:\n",
        "\n",
        "> I calculated the encodings from the training set only. If I include data from the validation and test sets into the encodings, I'll overestimate the model's performance. I should in general be vigilant to avoid leakage, that is, including any information from the validation and test sets into the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWqmz1AxhG_e",
        "colab_type": "text"
      },
      "source": [
        "### 2) Count encodings\n",
        "\n",
        "Here, encode the categorical features `['ip', 'app', 'device', 'os', 'channel']` using the count of each value in the data set. Using `CountEncoder` from the `category_encoders` library, I fit the encoding using the categorical feature columns defined in `cat_features`. Then apply the encodings to the train and validation sets, adding them as new columns with names suffixed `\"_count\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRQJQqZ7ixc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from category_encoders import CountEncoder\n",
        "\n",
        "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
        "train, valid, test = get_data_splits(clicks)\n",
        "\n",
        "# Create the count encoder\n",
        "count_enc = CountEncoder(cols=cat_features)\n",
        "\n",
        "# Learn encoding from the training set\n",
        "count_enc.fit(train[cat_features])\n",
        "\n",
        "# Apply encoding to the train and validation sets as new columns\n",
        "# Make sure to add `_count` as a suffix to the new columns\n",
        "train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))\n",
        "valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oERutgGqizvT",
        "colab_type": "text"
      },
      "source": [
        "# Train the model on the encoded datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYc724Q3jAt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model on the encoded datasets\n",
        "# This can take around 30 seconds to complete\n",
        "_ = train_model(train_encoded, valid_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQkatZTjO3l",
        "colab_type": "text"
      },
      "source": [
        "Out[12]\n",
        "Training model!\n",
        "Validation AUC score: 0.9653051135205329\n",
        "\n",
        "Count encoding improved our model's score!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrTwNUj5jmTt",
        "colab_type": "text"
      },
      "source": [
        "### 4) Target encoding\n",
        "\n",
        "Next I try some supervised encodings that use the labels (the targets) to transform categorical features. The first one is target encoding. I create the target encoder from the `category_encoders` library. Then, I learn the encodings from the training dataset, and then apply the encodings to all the datasets and retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIdnRl8BlZyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
        "train, valid, test = get_data_splits(clicks)\n",
        "\n",
        "# Create the target encoder. You can find this easily by using tab completion.\n",
        "# Start typing ce. the press Tab to bring up a list of classes and functions.\n",
        "target_enc = ce.TargetEncoder(cols=cat_features)\n",
        "\n",
        "# Learn encoding from the training set. Use the 'is_attributed' column as the target.\n",
        "target_enc.fit(train[cat_features], train['is_attributed'])\n",
        "\n",
        "# Apply encoding to the train and validation sets as new columns\n",
        "# Make sure to add `_target` as a suffix to the new columns\n",
        "train_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
        "valid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
        "\n",
        "train.head()\n",
        "bst = train_model(train, valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mpr8belbgJ",
        "colab_type": "text"
      },
      "source": [
        "Out[13]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m8N7BDhlmuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model!\n",
        "Validation AUC score: 0.9622743228943659"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lP7j9xRlzOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = train_model(train_encoded, valid_encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVQJwFE5loNN",
        "colab_type": "text"
      },
      "source": [
        "Out[14]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__f3cg-l08k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model!\n",
        "Validation AUC score: 0.9540530347873288"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt5m9Fjzm5DL",
        "colab_type": "text"
      },
      "source": [
        "# Note here:\n",
        "Target encoding attempts to measure the population mean of the target for each level in a categorical feature. This means when there is less data per level, the estimated mean will be further away from the \"true\" mean, there will be more variance. There is little data per IP address so it's likely that the estimates are much noisier than for the other features. The model will rely heavily on this feature since it is extremely predictive. This causes it to make fewer splits on other features, and those features are fit on just the errors left over accounting for IP address. So, the model will perform very poorly when seeing new IP addresses that weren't in the training data (which is likely most new data). Going forward, we'll leave out the IP feature when trying different encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey35urFam8KL",
        "colab_type": "text"
      },
      "source": [
        "### 6) CatBoost Encoding\n",
        "\n",
        "The CatBoost encoder is supposed to working well with the LightGBM model. Encode the categorical features with `CatBoostEncoder` and train the model on the encoded data again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvYz_ZV0nJq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, valid, test = get_data_splits(clicks)\n",
        "\n",
        "# Create the CatBoost encoder\n",
        "cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n",
        "\n",
        "# Learn encoding from the training set\n",
        "cb_enc.fit(train[cat_features], train['is_attributed'])\n",
        "\n",
        "# Apply encoding to the train and validation sets as new columns\n",
        "# Make sure to add `_cb` as a suffix to the new columns\n",
        "train_encoded = train.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
        "valid_encoded = valid.join(cb_enc.transform(valid[cat_features]).add_suffix('_cb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61en1jUTpD6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = train_model(train, valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wclBMAAHpWav",
        "colab_type": "text"
      },
      "source": [
        "Out[15]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4J-Lk9ApQ35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model!\n",
        "Validation AUC score: 0.9622743228943659"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrUD-q1apRMm",
        "colab_type": "text"
      },
      "source": [
        "The CatBoost encodings work the best, so I'll keep those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorGEQVlpkpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = cb_enc.transform(clicks[cat_features])\n",
        "for col in encoded:\n",
        "    clicks.insert(len(clicks.columns), col + '_cb', encoded[col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvUvIEqWpmVT",
        "colab_type": "text"
      },
      "source": [
        "Now I am ready to generate completely new features from the data itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhxXT-WzX1B",
        "colab_type": "text"
      },
      "source": [
        "I'll create new features from the existing data. Again I'll compare the score lift for each new feature compared to a baseline model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLzplSwPzsFE",
        "colab_type": "text"
      },
      "source": [
        "### 1) Add interaction features\n",
        "\n",
        "Here I add interaction features for each pair of categorical features (ip, app, device, os, channel). The easiest way to iterate through the pairs of features is with `itertools.combinations`. For each new column, join the values as strings with an underscore, so 13 and 47 would become `\"13_47\"`. As I add the new columns to the dataset, be sure to label encode the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb_2o1pN1WXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
        "interactions = pd.DataFrame(index=clicks.index)\n",
        "for col1, col2 in itertools.combinations(cat_features, 2):\n",
        "        new_col_name = '_'.join([col1, col2])\n",
        "\n",
        "        # Convert to strings and combine\n",
        "        new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str)\n",
        "\n",
        "        encoder = preprocessing.LabelEncoder()\n",
        "        interactions[new_col_name] = encoder.fit_transform(new_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX25xJCW1cfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clicks = clicks.join(interactions)\n",
        "print(\"Score with interactions\")\n",
        "train, valid, test = get_data_splits(clicks)\n",
        "_ = train_model(train, valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B7L5Xdz2vcM",
        "colab_type": "text"
      },
      "source": [
        "Out[20]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7dTAWW02rDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Score with interactions\n",
        "Training model. Hold on a minute to see the validation score\n",
        "Validation AUC score: 0.9626212895350978"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhR7mZu315XD",
        "colab_type": "text"
      },
      "source": [
        "# Generating numerical features\n",
        "\n",
        "Adding interactions is a quick way to create more categorical features from the data. It's also effective to create new numerical features, I'll typically get a lot of improvement in the model. This takes a bit of brainstorming and experimentation to find features that work well.\n",
        "\n",
        "For this case I'm going to implement functions that operate on Pandas Series. It can take multiple minutes to run these functions on the entire data set so instead I'll provide feedback by running my function on a smaller dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fySVNvyY2XXE",
        "colab_type": "text"
      },
      "source": [
        "### 2) Number of events in the past six hours\n",
        "\n",
        "The first feature I create is the number of events from the same IP in the last six hours. It's likely that someone who is visiting often will download the app.\n",
        "\n",
        "Implement a function `count_past_events` that takes a Series of click times (timestamps) and returns another Series with the number of events in the last hour. **Tip:** The `rolling` method is useful for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVoRDVto3393",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    def count_past_events(series, time_window='6H'):\n",
        "        series = pd.Series(series.index, index=series)\n",
        "        # Subtract 1 so the current event isn't counted\n",
        "        past_events = series.rolling(time_window).count() - 1\n",
        "        return past_events"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNFOQcOX4DRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading in from saved Parquet file\n",
        "past_events = pd.read_parquet('../input/feature-engineering-data/past_6hr_events.pqt')\n",
        "clicks['ip_past_6hr_counts'] = past_events\n",
        "\n",
        "train, valid, test = get_data_splits(clicks)\n",
        "_ = train_model(train, valid, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAyEsAjm4tcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model. Hold on a minute to see the validation score\n",
        "Validation AUC score: 0.9647255487084245"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOvW5dc4SX8",
        "colab_type": "text"
      },
      "source": [
        "### 3) Time since last event\n",
        "\n",
        "Implement a function `time_diff` that calculates the time since the last event in seconds from a Series of timestamps. This will be ran like so:\n",
        "\n",
        "```python\n",
        "timedeltas = clicks.groupby('ip')['click_time'].transform(time_diff)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqsfZq6E5Qjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def time_diff(series):\n",
        "    \"\"\" Returns a series with the time since the last timestamp in seconds \"\"\"\n",
        "    return series.diff().dt.total_seconds()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDZGo2jn53JS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading in from saved Parquet file\n",
        "past_events = pd.read_parquet('../input/feature-engineering-data/time_deltas.pqt')\n",
        "clicks['past_events_6hr'] = past_events\n",
        "\n",
        "train, valid, test = get_data_splits(clicks.join(past_events))\n",
        "_ = train_model(train, valid, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyrGqIm06KFy",
        "colab_type": "text"
      },
      "source": [
        "Out[21]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbmOY9aq6Isb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model. Hold on a minute to see the validation score\n",
        "Validation AUC score: 0.9651116624672765"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8xXpICq59Pd",
        "colab_type": "text"
      },
      "source": [
        "### 4) Number of previous app downloads\n",
        "\n",
        "It's likely that if a visitor downloaded an app previously, it'll affect the likelihood they'll download one again. Implement a function `previous_attributions` that returns a Series with the number of times an app has been download (`'is_attributed' == 1`) before the current event."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNeYxJze62ET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def previous_attributions(series):\n",
        "    \"\"\" Returns a series with the \"\"\"\n",
        "    sums = series.expanding(min_periods=2).sum() - series\n",
        "    return sums\n",
        "\n",
        "# Run & check my work"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqcY5rSS6_eC",
        "colab_type": "text"
      },
      "source": [
        "Again loading pre-computed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCT7JmNh7BCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading in from saved Parquet file\n",
        "past_events = pd.read_parquet('../input/feature-engineering-data/downloads.pqt')\n",
        "clicks['ip_past_6hr_counts'] = past_events"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F7igxl7Hgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, valid, test = get_data_splits(clicks)\n",
        "_ = train_model(train, valid, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2YfbAQn7MCW",
        "colab_type": "text"
      },
      "source": [
        "Out[23]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHtzcn1x7l9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model. Hold on a minute to see the validation score\n",
        "Validation AUC score: 0.965236652054989"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvvpwd8d8Gb0",
        "colab_type": "text"
      },
      "source": [
        "I frequently want to pare these created features down for modeling. Thus, up next is Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAqnidrk8Z3q",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection\n",
        "To find the most important features in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmOHE-7a84Px",
        "colab_type": "text"
      },
      "source": [
        "Note here: Often I have seen hundreds and thousands of features after various encodings and feature generation. This can lead to two problems. First, the more features I have, the more likely I am to overfit to the training and validation sets. This will cause my model to perform worse at generalizing to new data.\n",
        "\n",
        "Secondly, the more features I have, the longer it will take to train my model and optimize hyperparameters. Also, when building user-facing products, I want to make inference as fast as possible. Using fewer features can speed up inference at the cost of predictive performance.\n",
        "\n",
        "To help with these issues, I want to use feature selection techniques to keep the most informative features for your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAX7Wrdm-kxb",
        "colab_type": "text"
      },
      "source": [
        "# Univariate Feature Selection\n",
        "\n",
        "The simplest and fastest methods are based on univariate statistical tests. For each feature, measure how strongly the target depends on the feature using a statistical test like ùúí2\n",
        "\n",
        "or ANOVA.\n",
        "\n",
        "From the scikit-learn feature selection module, feature_selection.SelectKBest returns the K best features given some scoring function. For our classification problem, the module provides three different scoring functions: ùúí2\n",
        "\n",
        ", ANOVA F-value, and the mutual information score. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. The mutual information score is nonparametric and so can capture nonlinear relationships.\n",
        "\n",
        "With SelectKBest, I define the number of features to keep, based on the score from the scoring function. Using .fit_transform(features, target) I get back an array with only the selected features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLL-HeNmDMGi",
        "colab_type": "text"
      },
      "source": [
        "# Note here: Which data to use for feature selection?\n",
        "\n",
        "Since many feature selection methods require calculating statistics from the dataset, should you use all the data for feature selection?\n",
        "\n",
        "Answer:: Including validation and test data within the feature selection is a source of leakage. You'll want to perform feature selection on the train set only, then use the results there to remove features from the validation and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIuZtkB_FLWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
        "train, valid, test = get_data_splits(clicks)\n",
        "\n",
        "# Do feature extraction on the training data only!\n",
        "selector = SelectKBest(f_classif, k=40)\n",
        "X_new = selector.fit_transform(train[feature_cols], train['is_attributed'])\n",
        "\n",
        "    # Get back the features we've kept, zero out all other features\n",
        "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
        "                                 index=train.index, \n",
        "                                 columns=feature_cols)\n",
        "\n",
        "    # Dropped columns have values of all 0s, so var is 0, drop them\n",
        "dropped_columns = selected_features.columns[selected_features.var() == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAN8E9Rz8eXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = train_model(train.drop(dropped_columns, axis=1), \n",
        "                valid.drop(dropped_columns, axis=1),\n",
        "                test.drop(dropped_columns, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XC61Oo3FblJ",
        "colab_type": "text"
      },
      "source": [
        "Out[30]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwYxgCzjFaR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model!\n",
        "Validation AUC score: 0.9625481759576047"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIvTC35TF0Ft",
        "colab_type": "text"
      },
      "source": [
        "# Use L1 regularization for feature selection\n",
        "\n",
        "Now I try a more powerful approach using L1 regularization. Implementing a function `select_features_l1` that returns a list of features to keep.\n",
        "\n",
        "Using a `LogisticRegression` classifier model with an L1 penalty to select the features. For the model, set the random state to 7 and the regularization parameter to 0.1. Fit the model then use `SelectFromModel` to return a model with the selected features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmmir7ZYG67r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "def select_features_l1(X, y):\n",
        "    \"\"\" Return selected features using logistic regression with an L1 penalty \"\"\"\n",
        "    logistic = LogisticRegression(C=0.1, penalty=\"11\", random_state=7).fit(X,y)\n",
        "    model = SelectFromModel(logistic, prefit=True)\n",
        "\n",
        "    X_new = model.transform(X)\n",
        "\n",
        "        # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
        "    selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
        "                                        index=X.index,\n",
        "                                        columns=X.columns)\n",
        "\n",
        "        # Dropped columns have values of all 0s, keep other columns \n",
        "    cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
        "\n",
        "    return cols_to_keep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9EviZC4Idpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropped_columns = feature_cols.drop(selected_features)\n",
        "_ = train_model(train.drop(dropped_columns, axis=1), \n",
        "                valid.drop(dropped_columns, axis=1),\n",
        "                test.drop(dropped_columns, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgLXHD9PIkEY",
        "colab_type": "text"
      },
      "source": [
        "Out[34]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QNS0BWqIim6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Training model!\n",
        "Validation AUC score: 0.9658334271834417"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
