{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TalkingData AdTracking Fraud Detection Challenge.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheHouseOfVermeulens/wernervermeulen.github.io/blob/master/TalkingData_AdTracking_Fraud_Detection_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncD3w-hUGxSZ",
        "colab_type": "text"
      },
      "source": [
        "# DESCRIPTION: \n",
        "Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest\n",
        "mobile market in the world and therefore suffers from huge volumes of fradulent traffic.\n",
        "\n",
        "[TalkingData](https://www.talkingdata.com), China’s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.\n",
        "\n",
        "While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. Challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHw45xVHVRD",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION: \n",
        "The goal of the case is to predict if a user will download an app after clicking through an ad. For this case I have used a small sample of the data, dropping 99% of negative records (where the app wasn't downloaded) to make the target more balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zACj4cJGuOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "click_data = pd.read_csv('../input/feature-engineering-data/train_sample.csv',\n",
        "                         parse_dates=['click_time'])\n",
        "click_data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VigQbneaSlXw",
        "colab_type": "text"
      },
      "source": [
        "Out[1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIXTaxM4Q-mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      ip \t app \tdevice \tos \tchannel \tclick_time \t       attributed_time \tis_attributed\n",
        "0 \t89489 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:13:23 \tNaN                 \t0\n",
        "1 \t204158 \t35 \t 1 \t    13 \t  21 \t   2017-11-06 15:41:07 \t2017-11-07 08:17:19 \t1\n",
        "2 \t3437 \t   6 \t 1 \t    13 \t 459 \t   2017-11-06 15:42:32 \tNaN \t                0\n",
        "3 \t167543 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:56:17 \tNaN \t                0\n",
        "4 \t147509 \t 3 \t 1 \t    13 \t 379 \t   2017-11-06 15:57:01 \tNaN \t                0\n",
        "5 \t71421 \t15 \t 1 \t    13 \t 153 \t   2017-11-06 16:00:00 \tNaN \t                0\n",
        "6 \t76953 \t14 \t 1 \t    13 \t 379 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "7 \t187909 \t 2 \t 1 \t    25 \t 477 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "8 \t116779 \t 1 \t 1 \t     8 \t 150 \t   2017-11-06 16:00:01 \tNaN \t                0\n",
        "9 \t47857 \t 3 \t 1 \t    15 \t 205 \t   2017-11-06 16:00:01 \tNaN \t                0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUNRxXiXSDA1",
        "colab_type": "text"
      },
      "source": [
        "## Baseline Model\n",
        "\n",
        "The first thing I need to do is construct a baseline model. All new features, processing, encodings, and feature selection should improve upon this baseline model. First I need to do a bit of feature engineering before training the model itself.\n",
        "\n",
        "### 1) Features from timestamps\n",
        "From the timestamps, create features for the day, hour, minute and second. Store these as new integer columns `day`, `hour`, `minute`, and `second` in a new DataFrame `clicks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TQN6mj4Sb_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split up the times\n",
        "click_times = click_data['click_time']\n",
        "clicks['day'] = click_times.dt.day.astype('uint8')\n",
        "clicks['hour'] = click_times.dt.hour.astype('uint8')\n",
        "clicks['minute'] = click_times.dt.minute.astype('uint8')\n",
        "clicks['second'] = click_times.dt.second.astype('uint8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlwR60nkUBbw",
        "colab_type": "text"
      },
      "source": [
        "### 2) Label Encoding\n",
        "For each of the categorical features `['ip', 'app', 'device', 'os', 'channel']`, I use scikit-learn's `LabelEncoder` to create new features in the `clicks` DataFrame. The new column names should be the original column name with `'_labels'` appended, like `ip_labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awKXKURxUC-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Create new columns in clicks using preprocessing.LabelEncoder()\n",
        "for feature in cat_features:\n",
        "    encoded = label_encoder.fit_transform(clicks[feature])\n",
        "    clicks[feature + '_labels'] = encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NnekqmU8bB",
        "colab_type": "text"
      },
      "source": [
        "### 3) One-hot Encoding\n",
        "\n",
        "Now I have label encoded features, does it make sense to use one-hot encoding for the categorical variables ip, app, device, os, or channel?\n",
        "\n",
        "Answer: The ip column has 58,000 values, which means it will create an extremely sparse matrix with 58,000 columns. This many columns will make your model run very slow, so in general you want to avoid one-hot encoding features with many levels. LightGBM models work with label encoded features, so you don't actually need to one-hot encode the categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLZwYyIvVadt",
        "colab_type": "text"
      },
      "source": [
        "## Train, validation, and test sets\n",
        "With my baseline features ready, we need to split our data into training and validation sets. I should also hold out a test set to measure the final accuracy of the model.\n",
        "\n",
        "### 4) Train/test splits with time series data\n",
        "This is time series data. Are there any special considerations when creating train/test splits for time series? If so, what and why?\n",
        "\n",
        "Answer: Since our model is meant to predict events in the future, we must also validate the model on events in the future. If the data is mixed up between the training and test sets, then future data will leak in to the model and our validation results will overestimate the performance on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwecMtv8Wbpq",
        "colab_type": "text"
      },
      "source": [
        "### Create train/validation/test splits\n",
        "\n",
        "Here I'll create training, validation, and test splits. First, `clicks` DataFrame is sorted in order of increasing time. The first 80% of the rows are the train set, the next 10% are the validation set, and the last 10% are the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8xwIUeGVcTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_cols = ['day', 'hour', 'minute', 'second', \n",
        "                'ip_labels', 'app_labels', 'device_labels',\n",
        "                'os_labels', 'channel_labels']\n",
        "\n",
        "valid_fraction = 0.1\n",
        "clicks_srt = clicks.sort_values('click_time')\n",
        "valid_rows = int(len(clicks_srt) * valid_fraction)\n",
        "train = clicks_srt[:-valid_rows * 2]\n",
        "# valid size == test size, last two sections of the data\n",
        "valid = clicks_srt[-valid_rows * 2:-valid_rows]\n",
        "test = clicks_srt[-valid_rows:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjyYNDlYWvK0",
        "colab_type": "text"
      },
      "source": [
        "### Train with LightGBM\n",
        "\n",
        "Now I can create LightGBM dataset objects for each of the smaller datasets and train the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgiTvHy4W2Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
        "dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
        "dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
        "\n",
        "param = {'num_leaves': 64, 'objective': 'binary'}\n",
        "param['metric'] = 'auc'\n",
        "num_round = 1000\n",
        "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10x0V6yZXVtU",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model\n",
        "Finally, with the model trained, I'll evaluate it's performance on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amApWllSXW1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "ypred = bst.predict(test[feature_cols])\n",
        "score = metrics.roc_auc_score(test['is_attributed'], ypred)\n",
        "print(f\"Test score: {score}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZeBchTXjJP",
        "colab_type": "text"
      },
      "source": [
        "Out[17]\n",
        "Test score: 0.9726727334566094"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CVWl5uSX3gZ",
        "colab_type": "text"
      },
      "source": [
        "This is my baseline score for the model. When I transform features, add new ones, or perform feature selection, I should be improving on this score. However, since this is the test set, I only want to look at it at the end of all our manipulations. At the very end of this case I'll look at the test score again to see if I've improved on the baseline model."
      ]
    }
  ]
}